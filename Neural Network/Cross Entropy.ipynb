{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# implement the stochastic gradient descent learning algorithm for a FNN.\n",
    "# Gradients are calculated using backpropagation.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Import data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    " \n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "\n",
    "    def train(self, training_data_size, epochs, mini_batch_size, eta, lmbda, evaluation_data_size,\n",
    "              monitor_training_cost=True,\n",
    "              monitor_training_accuracy=True,\n",
    "              monitor_evaluation_cost=True,\n",
    "              monitor_evaluation_accuracy=True):\n",
    "\n",
    "        training_cost, training_accuracy = [], []\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        \n",
    "        evaluation_data = mnist.test.next_batch(evaluation_data_size, fake_data=False)\n",
    "        evaluation_data = [(np.transpose([x]), np.transpose([y])) \n",
    "                           for x, y in zip(evaluation_data[0], evaluation_data[1])]\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            for i in range(0, training_data_size, mini_batch_size):\n",
    "                training_data = mnist.train.next_batch(mini_batch_size, fake_data=False)\n",
    "                training_data = [(np.transpose([x]), np.transpose([y]))\n",
    "                                 for x, y in zip(training_data[0], training_data[1])]\n",
    "                self.train_mini_batch(training_data, eta, lmbda, training_data_size)\n",
    "        \n",
    "            print(\"Epoch %s training complete\" % j)\n",
    "\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print(\"Cost on training data: {}\".format(cost))\n",
    "                \n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on training data: {} / {}\".format(accuracy, mini_batch_size))\n",
    "                \n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda)\n",
    "                evaluation_cost.append(cost)\n",
    "                print(\"Cost on evaluation data: {}\".format(cost))\n",
    "                \n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on evaluation data: {} / {}\".format(accuracy,\n",
    "                                                                    evaluation_data_size))\n",
    "\n",
    "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
    "\n",
    "    def train_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        '''Update the network's weights and biases by applying gradient\n",
    "        descent using backpropagation to a single mini batch.  The\n",
    "        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\n",
    "        learning rate, ``lmbda`` is the regularization parameter, and\n",
    "        ``n`` is the total size of the training data set.\n",
    "\n",
    "        '''\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "\n",
    "        self.weights = [(1 - eta * (lmbda / n)) * w - (eta / len(mini_batch)) * nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta / len(mini_batch)) * nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        '''Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\n",
    "        '''\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "\n",
    "        # backward pass\n",
    "        delta = self.cost_delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l - 1].transpose())\n",
    "\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def cost(self, a, y):\n",
    "        return np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log(1 - a)))\n",
    "    \n",
    "    def cost_delta(self, z, a, y):\n",
    "        return a - y\n",
    "    \n",
    "    def total_cost(self, data, lmbda):\n",
    "        '''Return the total cost for the data set ``data``.'''\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            cost += self.cost(a, y) / len(data)\n",
    "            cost += 0.5 * (lmbda / len(data)) * sum(np.linalg.norm(w) ** 2\n",
    "                                                    for w in self.weights)\n",
    "        return cost\n",
    "    \n",
    "    def accuracy(self, data):\n",
    "        '''Return the number of inputs in ``data`` for which the neural\n",
    "        network outputs the correct result. The neural network's\n",
    "        output is assumed to be the index of whichever neuron in the\n",
    "        final layer has the highest activation.\n",
    "        '''\n",
    "        results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                   for (x, y) in data]\n",
    "\n",
    "        result_accuracy = sum(int(x == y) for (x, y) in results)\n",
    "        return result_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 5857.6303135547405\n",
      "Accuracy on training data: 9 / 10\n",
      "Cost on evaluation data: 5857.590773441423\n",
      "Accuracy on evaluation data: 9063 / 10000\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 5755.524626713724\n",
      "Accuracy on training data: 9 / 10\n",
      "Cost on evaluation data: 5755.625013026632\n",
      "Accuracy on evaluation data: 9225 / 10000\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 5645.450276604893\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 5645.792908134097\n",
      "Accuracy on evaluation data: 9313 / 10000\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 5540.591674595329\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 5540.940773963129\n",
      "Accuracy on evaluation data: 9336 / 10000\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 5437.453651747849\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 5437.669930291051\n",
      "Accuracy on evaluation data: 9379 / 10000\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 5335.422805668291\n",
      "Accuracy on training data: 9 / 10\n",
      "Cost on evaluation data: 5335.473834974612\n",
      "Accuracy on evaluation data: 9408 / 10000\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 5234.83314261006\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 5235.174582703417\n",
      "Accuracy on evaluation data: 9424 / 10000\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 5138.266016371442\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 5138.5952302731785\n",
      "Accuracy on evaluation data: 9442 / 10000\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 5042.596562771032\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 5042.871859116317\n",
      "Accuracy on evaluation data: 9461 / 10000\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 4954.643195990667\n",
      "Accuracy on training data: 9 / 10\n",
      "Cost on evaluation data: 4954.74475152685\n",
      "Accuracy on evaluation data: 9473 / 10000\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 4865.505965561501\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 4865.7816274679035\n",
      "Accuracy on evaluation data: 9498 / 10000\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 4782.391490873182\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 4782.713932641098\n",
      "Accuracy on evaluation data: 9487 / 10000\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 4703.1754067788\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 4703.354852828719\n",
      "Accuracy on evaluation data: 9469 / 10000\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 4628.032430776036\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 4628.159477190206\n",
      "Accuracy on evaluation data: 9528 / 10000\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 4550.560686261007\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 4550.84097658479\n",
      "Accuracy on evaluation data: 9516 / 10000\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 4475.306361179231\n",
      "Accuracy on training data: 8 / 10\n",
      "Cost on evaluation data: 4474.977440744241\n",
      "Accuracy on evaluation data: 9516 / 10000\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 4408.854259769085\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 4409.102774560922\n",
      "Accuracy on evaluation data: 9524 / 10000\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 4342.791240574633\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 4343.079461081713\n",
      "Accuracy on evaluation data: 9541 / 10000\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 4279.092037641018\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 4279.351781322067\n",
      "Accuracy on evaluation data: 9528 / 10000\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 4218.408553178745\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 4218.675937638641\n",
      "Accuracy on evaluation data: 9550 / 10000\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 4160.393127816198\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 4160.544897180791\n",
      "Accuracy on evaluation data: 9532 / 10000\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 4102.625352555377\n",
      "Accuracy on training data: 9 / 10\n",
      "Cost on evaluation data: 4102.11164515174\n",
      "Accuracy on evaluation data: 9546 / 10000\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 4047.2685604498724\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 4047.5177497627674\n",
      "Accuracy on evaluation data: 9567 / 10000\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 3996.2565981389557\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 3996.531279570858\n",
      "Accuracy on evaluation data: 9532 / 10000\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 3946.82902799377\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 3947.0870722462537\n",
      "Accuracy on evaluation data: 9566 / 10000\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 3897.8864691111667\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 3898.16580596521\n",
      "Accuracy on evaluation data: 9575 / 10000\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 3853.8641835273966\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 3854.146772226771\n",
      "Accuracy on evaluation data: 9550 / 10000\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 3807.297518444703\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 3807.4999793964776\n",
      "Accuracy on evaluation data: 9542 / 10000\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 3764.8912521226503\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 3765.1703440469078\n",
      "Accuracy on evaluation data: 9560 / 10000\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 3723.641246249623\n",
      "Accuracy on training data: 10 / 10\n",
      "Cost on evaluation data: 3723.8990500319846\n",
      "Accuracy on evaluation data: 9554 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([5857.5907734414232,\n",
       "  5755.6250130266317,\n",
       "  5645.7929081340972,\n",
       "  5540.9407739631288,\n",
       "  5437.6699302910511,\n",
       "  5335.4738349746121,\n",
       "  5235.1745827034174,\n",
       "  5138.5952302731785,\n",
       "  5042.871859116317,\n",
       "  4954.7447515268505,\n",
       "  4865.7816274679035,\n",
       "  4782.7139326410979,\n",
       "  4703.3548528287192,\n",
       "  4628.1594771902064,\n",
       "  4550.84097658479,\n",
       "  4474.977440744241,\n",
       "  4409.1027745609217,\n",
       "  4343.0794610817129,\n",
       "  4279.3517813220669,\n",
       "  4218.6759376386408,\n",
       "  4160.5448971807909,\n",
       "  4102.11164515174,\n",
       "  4047.5177497627674,\n",
       "  3996.5312795708578,\n",
       "  3947.0870722462537,\n",
       "  3898.16580596521,\n",
       "  3854.1467722267712,\n",
       "  3807.4999793964776,\n",
       "  3765.1703440469078,\n",
       "  3723.8990500319846],\n",
       " [9063,\n",
       "  9225,\n",
       "  9313,\n",
       "  9336,\n",
       "  9379,\n",
       "  9408,\n",
       "  9424,\n",
       "  9442,\n",
       "  9461,\n",
       "  9473,\n",
       "  9498,\n",
       "  9487,\n",
       "  9469,\n",
       "  9528,\n",
       "  9516,\n",
       "  9516,\n",
       "  9524,\n",
       "  9541,\n",
       "  9528,\n",
       "  9550,\n",
       "  9532,\n",
       "  9546,\n",
       "  9567,\n",
       "  9532,\n",
       "  9566,\n",
       "  9575,\n",
       "  9550,\n",
       "  9542,\n",
       "  9560,\n",
       "  9554],\n",
       " [5857.6303135547405,\n",
       "  5755.5246267137236,\n",
       "  5645.4502766048927,\n",
       "  5540.5916745953291,\n",
       "  5437.4536517478491,\n",
       "  5335.4228056682914,\n",
       "  5234.8331426100603,\n",
       "  5138.2660163714418,\n",
       "  5042.596562771032,\n",
       "  4954.6431959906668,\n",
       "  4865.5059655615014,\n",
       "  4782.3914908731822,\n",
       "  4703.1754067787997,\n",
       "  4628.0324307760357,\n",
       "  4550.5606862610066,\n",
       "  4475.3063611792313,\n",
       "  4408.8542597690848,\n",
       "  4342.7912405746329,\n",
       "  4279.0920376410177,\n",
       "  4218.408553178745,\n",
       "  4160.3931278161981,\n",
       "  4102.6253525553766,\n",
       "  4047.2685604498724,\n",
       "  3996.2565981389557,\n",
       "  3946.8290279937701,\n",
       "  3897.8864691111667,\n",
       "  3853.8641835273966,\n",
       "  3807.2975184447032,\n",
       "  3764.8912521226503,\n",
       "  3723.6412462496228],\n",
       " [9,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  8,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network([784, 30, 10])\n",
    "net.train(50000, 30, 10, 0.3, 0.5, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

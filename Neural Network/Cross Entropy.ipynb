{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# implement the stochastic gradient descent learning algorithm for a FNN.\n",
    "# Gradients are calculated using backpropagation.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Import data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    " \n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "\n",
    "    def train(self, training_data_size, epochs, mini_batch_size, eta, lmbda, evaluation_data_size,\n",
    "              monitor_training_cost=True,\n",
    "              monitor_training_accuracy=True,\n",
    "              monitor_evaluation_cost=True,\n",
    "              monitor_evaluation_accuracy=True):\n",
    "\n",
    "        training_cost, training_accuracy = [], []\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        \n",
    "        evaluation_data = mnist.test.next_batch(evaluation_data_size, fake_data=False)\n",
    "        evaluation_data = [(np.transpose([x]), np.transpose([y])) \n",
    "                           for x, y in zip(evaluation_data[0], evaluation_data[1])]\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            for i in range(0, training_data_size, mini_batch_size):\n",
    "                mini_batch = mnist.train.next_batch(mini_batch_size, fake_data=False)\n",
    "                training_data = [(np.transpose([x]), np.transpose([y]))\n",
    "                                 for x, y in zip(mini_batch[0], mini_batch[1])]\n",
    "                self.train_mini_batch(training_data, eta, lmbda, training_data_size)\n",
    "        \n",
    "            print(\"Epoch %s training complete\" % j)\n",
    "\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print(\"Cost on training data: {}\".format(cost))\n",
    "                \n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on training data: {} / {}\".format(accuracy, training_data_size))\n",
    "                \n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda)\n",
    "                evaluation_cost.append(cost)\n",
    "                print(\"Cost on evaluation data: {}\".format(cost))\n",
    "                \n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data), n_data))\n",
    "\n",
    "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
    "\n",
    "    def train_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        '''Update the network's weights and biases by applying gradient\n",
    "        descent using backpropagation to a single mini batch.  The\n",
    "        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\n",
    "        learning rate, ``lmbda`` is the regularization parameter, and\n",
    "        ``n`` is the total size of the training data set.\n",
    "\n",
    "        '''\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "\n",
    "        self.weights = [(1 - eta * (lmbda / n)) * w - (eta / len(mini_batch)) * nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta / len(mini_batch)) * nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        '''Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\n",
    "        '''\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "\n",
    "        # backward pass\n",
    "        delta = self.cost_delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l - 1].transpose())\n",
    "\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def cost(self, a, y):\n",
    "        return np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log(1 - a)))\n",
    "    \n",
    "    def cost_delta(self, z, a, y):\n",
    "        return a - y\n",
    "    \n",
    "    def total_cost(self, data, lmbda):\n",
    "        '''Return the total cost for the data set ``data``.  The flag\n",
    "        ``convert`` should be set to False if the data set is the\n",
    "        training data (the usual case), and to True if the data set is\n",
    "        the validation or test data.  See comments on the similar (but\n",
    "        reversed) convention for the ``accuracy`` method, above.\n",
    "        '''\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            cost += self.cost(a, y) / len(data)\n",
    "            cost += 0.5 * (lmbda / len(data)) * sum(np.linalg.norm(w) ** 2\n",
    "                                                    for w in self.weights)\n",
    "        return cost\n",
    "    \n",
    "    def accuracy(self, data):\n",
    "        '''Return the number of inputs in ``data`` for which the neural\n",
    "        network outputs the correct result. The neural network's\n",
    "        output is assumed to be the index of whichever neuron in the\n",
    "        final layer has the highest activation.\n",
    "\n",
    "        The flag ``convert`` should be set to False if the data set is\n",
    "        validation or test data (the usual case), and to True if the\n",
    "        data set is the training data. The need for this flag arises\n",
    "        due to differences in the way the results ``y`` are\n",
    "        represented in the different data sets.  In particular, it\n",
    "        flags whether we need to convert between the different\n",
    "        representations.  It may seem strange to use different\n",
    "        representations for the different data sets.  Why not use the\n",
    "        same representation for all three data sets?  It's done for\n",
    "        efficiency reasons -- the program usually evaluates the cost\n",
    "        on the training data and the accuracy on other data sets.\n",
    "        These are different types of computations, and using different\n",
    "        representations speeds things up.  More details on the\n",
    "        representations can be found in\n",
    "        mnist_loader.load_data_wrapper.\n",
    "        '''\n",
    "        results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                   for (x, y) in data]\n",
    "\n",
    "        result_accuracy = sum(int(x == y) for (x, y) in results)\n",
    "        return result_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 6155.89305344551\n",
      "Accuracy on training data: 8 / 50000\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 6073.348342935709\n",
      "Accuracy on training data: 9 / 50000\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 5988.621345071477\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 5893.45654786796\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 5802.44139955915\n",
      "Accuracy on training data: 9 / 50000\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 5708.35915378561\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 5621.047309108653\n",
      "Accuracy on training data: 9 / 50000\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 5530.505616172283\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 5446.649012302032\n",
      "Accuracy on training data: 9 / 50000\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 5364.201001868772\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 5283.007581691149\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 5205.413862740198\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 5132.995701758323\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 5062.660431171121\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 4990.146425895723\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 4926.862189370598\n",
      "Accuracy on training data: 9 / 50000\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 4866.154845272362\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 4807.180304535211\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 4749.590964148787\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 4693.071818893849\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 4640.56918326832\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 4591.520207843823\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 4544.490428770177\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 4493.464866453836\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 4449.698460790418\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 4406.902473112352\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 4362.442246675567\n",
      "Accuracy on training data: 10 / 50000\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 4324.5088515663665\n",
      "Accuracy on training data: 10 / 50000\n"
     ]
    }
   ],
   "source": [
    "net = Network([784, 30, 30, 10])\n",
    "net.train(50000, 300, 10, 0.3, 0.5, 10000,\n",
    "          monitor_evaluation_cost=False,\n",
    "          monitor_evaluation_accuracy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
